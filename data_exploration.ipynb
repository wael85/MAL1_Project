{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load the data and initial inspection\n",
    "# ----------------------------------------------\n",
    "# Load data (corrected file path)\n",
    "data = pd.read_csv(r'C:\\Users\\bhupa\\OneDrive - ViaUC\\Semester6\\MAL1\\final\\MAL1_Project\\archive\\all_stocks_5yr.csv')\n",
    "\n",
    "# Convert date to datetime\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "print(\"Missing Values:\\n\", missing_values)\n",
    "\n",
    "# Step 2: Identify target variable\n",
    "# ---------------------------------\n",
    "# For this analysis, we assume 'close' is the target variable for supervised learning tasks\n",
    "target = 'close'\n",
    "\n",
    "# Step 3: Visualize the data\n",
    "# ---------------------------\n",
    "# Plotting time series of prices for a specific stock (e.g., 'AAL')\n",
    "stock_data = data[data['Name'] == 'AAL']\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(stock_data['date'], stock_data['close'], label='Close')\n",
    "plt.plot(stock_data['date'], stock_data['open'], label='Open')\n",
    "plt.plot(stock_data['date'], stock_data['high'], label='High')\n",
    "plt.plot(stock_data['date'], stock_data['low'], label='Low')\n",
    "plt.legend()\n",
    "plt.title('Stock Prices Over Time for AAL')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.show()\n",
    "\n",
    "# Plotting volume over time for the same stock\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(stock_data['date'], stock_data['volume'], label='Volume')\n",
    "plt.legend()\n",
    "plt.title('Trading Volume Over Time for AAL')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Volume')\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Study correlations between features\n",
    "# --------------------------------------------\n",
    "# Correlation matrix for the specific stock\n",
    "corr_matrix = stock_data[['open', 'high', 'low', 'close', 'volume']].corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix for AAL')\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Identify promising transformations\n",
    "# -------------------------------------------\n",
    "# Log transformation of volume if skewed\n",
    "if stock_data['volume'].skew() > 1:\n",
    "    stock_data['log_volume'] = np.log(stock_data['volume'])\n",
    "\n",
    "# Example of creating lag feature\n",
    "stock_data['prev_close'] = stock_data['close'].shift(1)\n",
    "\n",
    "# Example of moving average\n",
    "stock_data['7_day_ma'] = stock_data['close'].rolling(window=7).mean()\n",
    "\n",
    "# Display first few rows to verify transformations\n",
    "print(stock_data.head())\n",
    "\n",
    "# Save the transformed dataset to a new CSV file for further use\n",
    "stock_data.to_csv('transformed_AAL_stock_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Union, Optional, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy.stats import zscore\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from prophet import Prophet\n",
    "import pmdarima as pm\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_data(filepath: Union[str, Path]) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Load stock data from a CSV file.\"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(filepath)\n",
    "        data['date'] = pd.to_datetime(data['date'])\n",
    "        required_columns = {'date', 'open', 'high', 'low', 'close', 'volume', 'Name'}\n",
    "        missing_columns = required_columns - set(data.columns)\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing columns: {missing_columns}\")\n",
    "        logger.info(\"Data loaded successfully.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "def explore_data(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Explore the data and log important characteristics.\"\"\"\n",
    "    exploration_report = []\n",
    "    \n",
    "    for col in data.columns:\n",
    "        col_type = data[col].dtype\n",
    "        missing_percentage = data[col].isnull().mean() * 100\n",
    "        unique_values = data[col].nunique()\n",
    "        outliers = ((np.abs(zscore(data[col])) > 3).sum()) if np.issubdtype(col_type, np.number) else 'N/A'\n",
    "        exploration_report.append({\n",
    "            \"Name\": col,\n",
    "            \"Type\": col_type,\n",
    "            \"Missing Percentage\": missing_percentage,\n",
    "            \"Unique Values\": unique_values,\n",
    "            \"Outliers\": outliers\n",
    "        })\n",
    "        \n",
    "        logger.info(f\"Feature '{col}' - Type: {col_type}, Missing: {missing_percentage:.2f}%, Unique: {unique_values}, Outliers: {outliers}\")\n",
    "    \n",
    "    exploration_df = pd.DataFrame(exploration_report)\n",
    "    logger.info(f\"Exploration Report:\\n{exploration_df}\")\n",
    "    return exploration_df\n",
    "\n",
    "def visualize_data(stock_data: pd.DataFrame, stock_name: str) -> None:\n",
    "    \"\"\"Visualize stock prices and trading volume over time.\"\"\"\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(stock_data['date'], stock_data['close'], label='Close')\n",
    "    plt.plot(stock_data['date'], stock_data['open'], label='Open')\n",
    "    plt.plot(stock_data['date'], stock_data['high'], label='High')\n",
    "    plt.plot(stock_data['date'], stock_data['low'], label='Low')\n",
    "    plt.legend()\n",
    "    plt.title(f'Stock Prices Over Time: {stock_name}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Price')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(stock_data['date'], stock_data['volume'], label='Volume')\n",
    "    plt.legend()\n",
    "    plt.title(f'Trading Volume Over Time: {stock_name}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Volume')\n",
    "    plt.show()\n",
    "\n",
    "def plot_correlation_matrix(stock_data: pd.DataFrame) -> None:\n",
    "    \"\"\"Plot the correlation matrix of the stock data features.\"\"\"\n",
    "    features = [col for col in stock_data.columns if col not in ['date', 'Name']]\n",
    "    corr_matrix = stock_data[features].corr()\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.show()\n",
    "\n",
    "def clean_data(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Fill missing values in the data.\"\"\"\n",
    "    data = data.fillna(method='ffill')\n",
    "    logger.info(\"Data cleaned with forward fill.\")\n",
    "    return data\n",
    "\n",
    "def calculate_rsi(prices: pd.Series, window: int = 14) -> pd.Series:\n",
    "    \"\"\"Calculate the Relative Strength Index (RSI) for a given series of prices.\"\"\"\n",
    "    delta = prices.diff()\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "    avg_gain = gain.rolling(window=window).mean()\n",
    "    avg_loss = loss.rolling(window=window).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def calculate_macd(prices: pd.Series, short_period: int = 12, long_period: int = 26, signal_period: int = 9) -> Tuple[pd.Series, pd.Series]:\n",
    "    \"\"\"Calculate the Moving Average Convergence Divergence (MACD) for a given series of prices.\"\"\"\n",
    "    short_ema = prices.ewm(span=short_period, adjust=False).mean()\n",
    "    long_ema = prices.ewm(span=long_period, adjust=False).mean()\n",
    "    macd = short_ema - long_ema\n",
    "    signal_line = macd.ewm(span=signal_period, adjust=False).mean()\n",
    "    return macd, signal_line\n",
    "\n",
    "def calculate_bollinger_bands(prices: pd.Series, window: int = 20, num_std: int = 2) -> Tuple[pd.Series, pd.Series]:\n",
    "    \"\"\"Calculate Bollinger Bands for a given series of prices.\"\"\"\n",
    "    rolling_mean = prices.rolling(window=window).mean()\n",
    "    rolling_std = prices.rolling(window=window).std()\n",
    "    upper_band = rolling_mean + (rolling_std * num_std)\n",
    "    lower_band = rolling_mean - (rolling_std * num_std)\n",
    "    return upper_band, lower_band\n",
    "\n",
    "def engineer_features(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Engineer features for the stock data.\"\"\"\n",
    "    if data['volume'].skew() > 1:\n",
    "        data['log_volume'] = np.log1p(data['volume'])\n",
    "    data['prev_close'] = data['close'].shift(1)\n",
    "    ma_windows = [7, 14, 21]\n",
    "    for window in ma_windows:\n",
    "        data[f'{window}_day_ma'] = data['close'].rolling(window=window).mean()\n",
    "    data['rsi'] = calculate_rsi(data['close'])\n",
    "    data['macd'], data['signal_line'] = calculate_macd(data['close'])\n",
    "    data['bollinger_upper'], data['bollinger_lower'] = calculate_bollinger_bands(data['close'])\n",
    "    data['pct_change'] = data['close'].pct_change()\n",
    "    data['volatility'] = data['close'].rolling(window=14).std()\n",
    "    \n",
    "    # Lag features\n",
    "    for lag in range(1, 4):\n",
    "        data[f'lag_{lag}'] = data['close'].shift(lag)\n",
    "        \n",
    "    data.dropna(inplace=True)\n",
    "    logger.info(\"Features engineered.\")\n",
    "    return data\n",
    "\n",
    "class OutlierHandler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold: float = 3.0):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None) -> 'OutlierHandler':\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X = X.copy()\n",
    "        for col in X.columns:\n",
    "            z_scores = zscore(X[col])\n",
    "            X[col] = np.where(np.abs(z_scores) > self.threshold, np.median(X[col]), X[col])\n",
    "        return X\n",
    "\n",
    "def scale_features(data: pd.DataFrame) -> Tuple[pd.DataFrame, RobustScaler]:\n",
    "    \"\"\"Scale features using RobustScaler.\"\"\"\n",
    "    scaler = RobustScaler()\n",
    "    features = ['open', 'high', 'low', 'close', 'volume']\n",
    "    data[features] = scaler.fit_transform(data[features])\n",
    "    logger.info(\"Features scaled.\")\n",
    "    return data, scaler\n",
    "\n",
    "def prepare_data(filepath: Union[str, Path], stock_name: str) -> Tuple[Optional[pd.DataFrame], Optional[RobustScaler]]:\n",
    "    \"\"\"Prepare data for modeling: load, clean, engineer features, and scale.\"\"\"\n",
    "    data = load_data(filepath)\n",
    "    if data is None:\n",
    "        return None, None\n",
    "    stock_data = data[data['Name'] == stock_name].copy()\n",
    "    stock_data = clean_data(stock_data)\n",
    "    stock_data = engineer_features(stock_data)\n",
    "    stock_data, scaler = scale_features(stock_data)\n",
    "    return stock_data, scaler\n",
    "\n",
    "def save_transformed_data(stock_data: pd.DataFrame, filename: Union[str, Path]) -> None:\n",
    "    \"\"\"Save the transformed stock data to a CSV file.\"\"\"\n",
    "    stock_data.to_csv(filename, index=False)\n",
    "    logger.info(f\"Transformed data saved to {filename}\")\n",
    "\n",
    "def prepare_time_series_data(data: pd.DataFrame, target: str = 'close') -> pd.DataFrame:\n",
    "    \"\"\"Prepare the data for time series modeling.\"\"\"\n",
    "    data.set_index('date', inplace=True)\n",
    "    data.sort_index(inplace=True)\n",
    "    return data\n",
    "\n",
    "def evaluate_model(true: pd.Series, predicted: np.ndarray) -> Tuple[float, float]:\n",
    "    \"\"\"Evaluate the model performance.\"\"\"\n",
    "    mse = mean_squared_error(true, predicted)\n",
    "    mae = mean_absolute_error(true, predicted)\n",
    "    return mse, mae\n",
    "\n",
    "def time_series_cv(model: Union[BaseEstimator, Sequential], X: pd.DataFrame, y: pd.Series, n_splits: int = 3) -> Tuple[float, float]:\n",
    "    \"\"\"Perform time series cross-validation.\"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    mse_scores, mae_scores = [], []\n",
    "\n",
    "    if hasattr(model, 'get_params'):  # Check if the model is a scikit-learn estimator\n",
    "        for train_idx, test_idx in tscv.split(X):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            mse_scores.append(mean_squared_error(y_test, y_pred))\n",
    "            mae_scores.append(mean_absolute_error(y_test, y_pred))\n",
    "    else:  # Handle non-scikit-learn models (e.g., LSTM)\n",
    "        for train_idx, test_idx in tscv.split(X):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "            X_train_reshaped = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "            model.fit(X_train_reshaped, y_train, epochs=50, batch_size=32, verbose=0, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "            X_test_reshaped = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "            y_pred = model.predict(X_test_reshaped)\n",
    "            mse_scores.append(mean_squared_error(y_test, y_pred.flatten()))\n",
    "            mae_scores.append(mean_absolute_error(y_test, y_pred.flatten()))\n",
    "\n",
    "    mse = np.mean(mse_scores)\n",
    "    mae = np.mean(mae_scores)\n",
    "    logger.info(f\"Cross-validation results - MSE: {mse:.4f}, MAE: {mae:.4f}\")\n",
    "    return mse, mae\n",
    "\n",
    "def train_prophet(data: pd.DataFrame) -> Tuple[Prophet, float, float]:\n",
    "    \"\"\"Train a Prophet model.\"\"\"\n",
    "    prophet_data = data.reset_index().rename(columns={'date': 'ds', 'close': 'y'})\n",
    "    model = Prophet(yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=True)\n",
    "    model.fit(prophet_data)\n",
    "    future = model.make_future_dataframe(periods=len(data), freq='D')\n",
    "    forecast = model.predict(future)\n",
    "    predicted = forecast['yhat'].values[-len(data):]\n",
    "    mse, mae = evaluate_model(data['close'], predicted)\n",
    "    logger.info(f\"Prophet model trained. MSE: {mse:.4f}, MAE: {mae:.4f}\")\n",
    "    return model, mse, mae\n",
    "\n",
    "def train_auto_arima(data: pd.DataFrame) -> Tuple[pm.ARIMA, float, float]:\n",
    "    \"\"\"Train an Auto-ARIMA model.\"\"\"\n",
    "    model = pm.auto_arima(data['close'], seasonal=True, m=12)\n",
    "    forecast = model.predict(n_periods=len(data))\n",
    "    mse, mae = evaluate_model(data['close'], forecast)\n",
    "    logger.info(f\"Auto-ARIMA model trained. MSE: {mse:.4f}, MAE: {mae:.4f}\")\n",
    "    return model, mse, mae\n",
    "\n",
    "def train_linear_regression(X: pd.DataFrame, y: pd.Series) -> Tuple[Ridge, float, float]:\n",
    "    \"\"\"Train a Linear Regression model with regularization.\"\"\"\n",
    "    model = Ridge(alpha=1.0)  # Using Ridge regression for regularization\n",
    "    model.fit(X, y)\n",
    "    mse, mae = time_series_cv(model, X, y)\n",
    "    logger.info(f\"Linear Regression model trained with Ridge regularization. MSE: {mse:.4f}, MAE: {mae:.4f}\")\n",
    "    return model, mse, mae\n",
    "\n",
    "def train_xgboost(X: pd.DataFrame, y: pd.Series) -> Tuple[XGBRegressor, float, float]:\n",
    "    \"\"\"Train an XGBoost model.\"\"\"\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 300, 500],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "    model = XGBRegressor()\n",
    "    search = RandomizedSearchCV(model, param_grid, cv=3, n_iter=10, scoring='neg_mean_squared_error', n_jobs=-1, random_state=42)\n",
    "    search.fit(X, y)\n",
    "    best_model = search.best_estimator_\n",
    "    mse, mae = time_series_cv(best_model, X, y)\n",
    "    logger.info(f\"XGBoost model trained with best params: {search.best_params_}. MSE: {mse:.4f}, MAE: {mae:.4f}\")\n",
    "    return best_model, mse, mae\n",
    "\n",
    "def train_lightgbm(X: pd.DataFrame, y: pd.Series) -> Tuple[LGBMRegressor, float, float]:\n",
    "    \"\"\"Train a LightGBM model.\"\"\"\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 300, 500],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'num_leaves': [31, 50, 70],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "    model = LGBMRegressor()\n",
    "    search = RandomizedSearchCV(model, param_grid, cv=3, n_iter=10, scoring='neg_mean_squared_error', n_jobs=-1, random_state=42, verbose=-1)  # Suppress detailed logs\n",
    "    search.fit(X, y)\n",
    "    best_model = search.best_estimator_\n",
    "    mse, mae = time_series_cv(best_model, X, y)\n",
    "    logger.info(f\"LightGBM model trained with best params: {search.best_params_}. MSE: {mse:.4f}, MAE: {mae:.4f}\")\n",
    "    return best_model, mse, mae\n",
    "\n",
    "def train_lstm(X: pd.DataFrame, y: pd.Series) -> Tuple[Sequential, float, float]:\n",
    "    \"\"\"Train an LSTM model.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(1, X.shape[1])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Bidirectional(LSTM(64)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    X_reshaped = X.values.reshape((X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "    model.fit(X_reshaped, y, epochs=100, batch_size=32, verbose=0, validation_split=0.2, callbacks=[early_stopping])\n",
    "    mse, mae = time_series_cv(model, X, y)\n",
    "    logger.info(f\"LSTM model trained. MSE: {mse:.4f}, MAE: {mae:.4f}\")\n",
    "    return model, mse, mae\n",
    "\n",
    "def train_svr(X: pd.DataFrame, y: pd.Series) -> Tuple[SVR, float, float]:\n",
    "    \"\"\"Train an SVR model.\"\"\"\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1.0, 10],\n",
    "        'epsilon': [0.01, 0.1, 0.2],\n",
    "        'kernel': ['rbf']\n",
    "    }\n",
    "    model = SVR()\n",
    "    search = RandomizedSearchCV(model, param_grid, cv=3, n_iter=10, scoring='neg_mean_squared_error', n_jobs=-1, random_state=42)\n",
    "    search.fit(X, y)\n",
    "    best_model = search.best_estimator_\n",
    "    mse, mae = time_series_cv(best_model, X, y)\n",
    "    logger.info(f\"SVR model trained with best params: {search.best_params_}. MSE: {mse:.4f}, MAE: {mae:.4f}\")\n",
    "    return best_model, mse, mae\n",
    "\n",
    "def select_features(X: pd.DataFrame, y: pd.Series, k: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"Select the top k features using SelectKBest.\"\"\"\n",
    "    selector = SelectKBest(f_regression, k=k)\n",
    "    selector.fit(X, y)\n",
    "    selected_features = X.columns[selector.get_support()]\n",
    "    logger.info(f\"Selected features: {selected_features}\")\n",
    "    return X[selected_features]\n",
    "\n",
    "def train_ensemble(models: list, X: pd.DataFrame, y: pd.Series) -> Tuple[VotingRegressor, float, float]:\n",
    "    \"\"\"Train an ensemble model.\"\"\"\n",
    "    ensemble = VotingRegressor(estimators=models)\n",
    "    mse, mae = time_series_cv(ensemble, X, y)\n",
    "    logger.info(f\"Ensemble model trained. MSE: {mse:.4f}, MAE: {mae:.4f}\")\n",
    "    return ensemble, mse, mae\n",
    "\n",
    "def compare_models(models: Dict[str, Tuple[Any, float, float]]) -> None:\n",
    "    \"\"\"Compare the performance of different models.\"\"\"\n",
    "    model_names, mse_scores, mae_scores = zip(*[(name, mse, mae) for name, (_, mse, mae) in models.items()])\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(model_names, mse_scores)\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Mean Squared Error (MSE)')\n",
    "    plt.title('Model Comparison - MSE')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(model_names, mae_scores)\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Mean Absolute Error (MAE)')\n",
    "    plt.title('Model Comparison - MAE')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "def analyze_feature_importance(model: Any, feature_names: list) -> None:\n",
    "    \"\"\"Analyze and plot feature importance for models that support it.\"\"\"\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.title('Feature Importances')\n",
    "        plt.bar(range(len(importances)), importances[indices], align='center')\n",
    "        plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=45)\n",
    "        plt.xlabel('Feature')\n",
    "        plt.ylabel('Importance')\n",
    "        plt.show()\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        importances = model.coef_\n",
    "        indices = np.argsort(np.abs(importances))[::-1]\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.title('Feature Importances')\n",
    "        plt.bar(range(len(importances)), importances[indices], align='center')\n",
    "        plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=45)\n",
    "        plt.xlabel('Feature')\n",
    "        plt.ylabel('Importance')\n",
    "        plt.show()\n",
    "    else:\n",
    "        logger.info(\"Model does not support feature importance analysis.\")\n",
    "\n",
    "def analyze_residuals(true: pd.Series, predicted: np.ndarray) -> None:\n",
    "    \"\"\"Analyze residuals of the model predictions.\"\"\"\n",
    "    residuals = true - predicted\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(residuals, bins=50)\n",
    "    plt.title('Residuals Distribution')\n",
    "    plt.xlabel('Residual')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(true, residuals)\n",
    "    plt.title('Residuals vs True Values')\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.axhline(0, color='r', linestyle='--')\n",
    "    plt.show()\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"Main function to execute the modeling process.\"\"\"\n",
    "    filepath = 'all_stocks_5yr.csv'\n",
    "    top_stocks = ['AAPL']\n",
    "\n",
    "    # Configure logging\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    logging.getLogger('cmdstanpy').setLevel(logging.WARNING)\n",
    "    logging.getLogger('prophet').setLevel(logging.WARNING)\n",
    "    logging.getLogger('lightgbm').setLevel(logging.WARNING)\n",
    "\n",
    "    for stock_name in top_stocks:\n",
    "        logger.info(f\"Analyzing {stock_name}:\")\n",
    "        stock_data, scaler = prepare_data(filepath, stock_name)\n",
    "        if stock_data is None:\n",
    "            logger.error(\"Error preparing data.\")\n",
    "            continue\n",
    "\n",
    "        # Data exploration\n",
    "        exploration_report = explore_data(stock_data)\n",
    "        \n",
    "        # Visualize the data\n",
    "        visualize_data(stock_data, stock_name)\n",
    "        plot_correlation_matrix(stock_data)\n",
    "        \n",
    "        # Save the transformed data\n",
    "        save_transformed_data(stock_data, f\"{stock_name}_transformed.csv\")\n",
    "\n",
    "        # Prepare time series data\n",
    "        time_series_data = prepare_time_series_data(stock_data)\n",
    "        X = time_series_data.drop(['close', 'Name'], axis=1)  # Drop 'close' and 'Name' columns\n",
    "        y = time_series_data['close']\n",
    "\n",
    "        # Select best features\n",
    "        X_selected = select_features(X, y)\n",
    "\n",
    "        # Train models and log their performance\n",
    "        models = {\n",
    "            'Prophet': train_prophet(time_series_data),\n",
    "            'Auto-ARIMA': train_auto_arima(time_series_data),\n",
    "            'Linear Regression': train_linear_regression(X_selected, y),\n",
    "            'XGBoost': train_xgboost(X_selected, y),\n",
    "            'LightGBM': train_lightgbm(X_selected, y),\n",
    "            'LSTM': train_lstm(X_selected, y),\n",
    "            'SVR': train_svr(X_selected, y),\n",
    "        }\n",
    "\n",
    "        logger.info(\"Model Performance:\")\n",
    "        for name, (_, mse, mae) in models.items():\n",
    "            logger.info(f\"{name} - MSE: {mse:.4f}, MAE: {mae:.4f}\")\n",
    "\n",
    "        # Compare model performances\n",
    "        compare_models(models)\n",
    "\n",
    "        # Train ensemble model (excluding LSTM as it is not sklearn compatible)\n",
    "        ensemble_model, ensemble_mse, ensemble_mae = train_ensemble([\n",
    "            ('linear', models['Linear Regression'][0]),\n",
    "            ('xgboost', models['XGBoost'][0]),\n",
    "            ('lightgbm', models['LightGBM'][0])\n",
    "        ], X_selected, y)\n",
    "        logger.info(f\"Ensemble - MSE: {ensemble_mse:.4f}, MAE: {ensemble_mae:.4f}\")\n",
    "\n",
    "        # Analyze feature importance for selected models\n",
    "        analyze_feature_importance(models['XGBoost'][0], X_selected.columns)\n",
    "        analyze_feature_importance(models['LightGBM'][0], X_selected.columns)\n",
    "        analyze_feature_importance(models['Linear Regression'][0], X_selected.columns)\n",
    "\n",
    "        # Analyze residuals for the ensemble model\n",
    "        ensemble_pred = ensemble_model.predict(X_selected)\n",
    "        analyze_residuals(y, ensemble_pred)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
